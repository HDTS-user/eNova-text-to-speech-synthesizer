{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Otdf2nhRTHWa",
   "metadata": {
    "id": "Otdf2nhRTHWa"
   },
   "source": [
    "## AWS Marketplace Product Usage Demonstration - Text to Speech Synthesis  Model Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4jid2S76jDL",
   "metadata": {
    "id": "p4jid2S76jDL"
   },
   "source": [
    "**eNova Text to Speech Synthesis**\n",
    "\n",
    "This sample notebook demonstrate how to use TTS sysnthesis using SSML. It demonstrate how to deploy it on Amazon Sagemaker.\n",
    "\n",
    "**Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "#### Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used:\n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to eNova Text to Speech model. If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "#### Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Set up the environment](#2.-Setup-the-environment)\n",
    "3. [Create the session](#3.-Create-the-session)\n",
    "4. [Create Model](#4.-Create-model)\n",
    "5. [Perform batch inference](#5.-Perform-batch-inference)\n",
    "    1. [Configure the input S3 bucket folder](#A.-Configure-the-input-S3-bucket-folder)\n",
    "    2. [Deploy the model](#B.-Deploy-the-model)\n",
    "\n",
    "6. [Clean-up](#Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "\n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fFvRXs8sUOqV",
   "metadata": {
    "id": "fFvRXs8sUOqV"
   },
   "source": [
    "<a name=\"1.-Subscribe-to-the-model-package\"></a>\n",
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SsyVm4w0UQEg",
   "metadata": {
    "id": "SsyVm4w0UQEg"
   },
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page DefenSight Botnet Detector.\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms.\n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Yg5ysM-AVfwp",
   "metadata": {
    "id": "Yg5ysM-AVfwp"
   },
   "outputs": [],
   "source": [
    "model_package_arn = 'arn:aws:sagemaker:us-east-1:822940408628:model-package/enova-text-speech-synthesis'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FTZBPRDHUV6Q",
   "metadata": {
    "id": "FTZBPRDHUV6Q"
   },
   "source": [
    "<a name=\"2.-Setup-the-environment\"></a>\n",
    "## 2. Set up the environment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9c0a3f",
   "metadata": {
    "id": "fa9c0a3f"
   },
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 prefixes\n",
    "common_prefix = \"hdts-sagemaker-testing\"\n",
    "batch_inference_input_prefix = \"tts/input\"\n",
    "batch_inference_output_prefix = \"tts/output\"\n",
    "from sagemaker.predictor import Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff55c1",
   "metadata": {
    "id": "07ff55c1"
   },
   "source": [
    "<a name=\"3.-Create-the-session\"></a>\n",
    "## 3. Create the session\n",
    "\n",
    "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our Amazon SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b061768e",
   "metadata": {
    "id": "b061768e",
    "outputId": "6b5240ec-dbc7-46be-f912-a3e47d0d2f53"
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650785a",
   "metadata": {
    "id": "e650785a"
   },
   "source": [
    "<a name=\"4.-Create-model\"></a>\n",
    "## 4. Create Model\n",
    "\n",
    "Now we use the above Model Package to create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a394d",
   "metadata": {
    "id": "b48a394d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32346e5",
   "metadata": {
    "id": "f32346e5"
   },
   "outputs": [],
   "source": [
    "model_name = 'enova_speech_synthesis'\n",
    "\n",
    "content_type = 'text/xml'\n",
    "\n",
    "batch_transform_inference_instance_type = 'ml.m5.large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a681e6ea",
   "metadata": {
    "id": "a681e6ea"
   },
   "outputs": [],
   "source": [
    "def predict_wrapper(endpoint, session):\n",
    "    return sage.predictor.Predictor(endpoint, session, content_type)\n",
    "\n",
    "# Create a deployable model from the model package\n",
    "model = ModelPackage(role=role,\n",
    "                    model_package_arn=model_package_arn,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    predictor_cls = predict_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5320447",
   "metadata": {
    "id": "b5320447"
   },
   "source": [
    "<a name=\"6.-Perform-batch-inference\"></a>\n",
    "## 6. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ef8ee",
   "metadata": {
    "id": "4f0ef8ee"
   },
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc50238",
   "metadata": {
    "id": "acc50238"
   },
   "source": [
    "<a name=\"A.-Configure-the-input-S3-bucket-folder\"></a>\n",
    "### A. Configure the input S3 bucket folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249cf7bb",
   "metadata": {
    "id": "249cf7bb"
   },
   "outputs": [],
   "source": [
    "transform_input_folder = 'input'\n",
    "transform_input = sagemaker_session.upload_data(transform_input_folder,common_prefix,batch_inference_input_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c860706",
   "metadata": {
    "id": "1c860706"
   },
   "source": [
    "<a name=\"B.-Deploy-the-model\"></a>\n",
    "### B. Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a302051b",
   "metadata": {
    "id": "a302051b",
    "outputId": "5b83bfbd-ef50-4e93-f9bb-4c7cfe2cf91b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: enova-text-speech-synthesis-2023-08-22-05-44-18-767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................\u001b[34mStarting the inference server with 2 workers.\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [9] [INFO] Starting gunicorn 21.2.0\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [9] [INFO] Listening at: unix:/tmp/gunicorn.sock (9)\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [9] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [12] [INFO] Booting worker with pid: 12\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [13] [INFO] Booting worker with pid: 13\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:50:38 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:50:38 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mLength of recieved data is 1180\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [22/Aug/2023:05:50:38 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [22/Aug/2023:05:50:38 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mLength of recieved data is 1180\u001b[0m\n",
      "\u001b[32m2023-08-22T05:50:39.007:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:51:05 +0000] \"POST /invocations HTTP/1.1\" 200 1262636 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mLength of recieved data is 30\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [22/Aug/2023:05:51:05 +0000] \"POST /invocations HTTP/1.1\" 200 1262636 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mLength of recieved data is 30\u001b[0m\n",
      "\u001b[34mLength of recieved data is 30\u001b[0m\n",
      "\u001b[35mLength of recieved data is 30\u001b[0m\n",
      "\u001b[34mdone\u001b[0m\n",
      "\u001b[34m== The entire request object ==\u001b[0m\n",
      "\u001b[34m<Request 'http://169.254.255.131:8080/invocations' [POST]>\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:51:09 +0000] \"POST /invocations HTTP/1.1\" 200 108588 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mLength of recieved data is 4397\u001b[0m\n",
      "\u001b[35mdone\u001b[0m\n",
      "\u001b[35m== The entire request object ==\u001b[0m\n",
      "\u001b[35m<Request 'http://169.254.255.131:8080/invocations' [POST]>\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [22/Aug/2023:05:51:09 +0000] \"POST /invocations HTTP/1.1\" 200 108588 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mLength of recieved data is 4397\u001b[0m\n",
      "\n",
      "\u001b[34mStarting the inference server with 2 workers.\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [9] [INFO] Starting gunicorn 21.2.0\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [9] [INFO] Listening at: unix:/tmp/gunicorn.sock (9)\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [9] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [12] [INFO] Booting worker with pid: 12\u001b[0m\n",
      "\u001b[34m[2023-08-22 05:50:30 +0000] [13] [INFO] Booting worker with pid: 13\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:50:38 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:50:38 +0000] \"GET /execution-parameters HTTP/1.1\" 404 2 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mLength of recieved data is 1180\u001b[0m\n",
      "\u001b[32m2023-08-22T05:50:39.007:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mdone\u001b[0m\n",
      "\u001b[34m== The entire request object ==\u001b[0m\n",
      "\u001b[34m<Request 'http://169.254.255.131:8080/invocations' [POST]>\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:51:05 +0000] \"POST /invocations HTTP/1.1\" 200 1262636 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mLength of recieved data is 30\u001b[0m\n",
      "\u001b[34mdone\u001b[0m\n",
      "\u001b[34m== The entire request object ==\u001b[0m\n",
      "\u001b[34m<Request 'http://169.254.255.131:8080/invocations' [POST]>\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:51:09 +0000] \"POST /invocations HTTP/1.1\" 200 108588 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34mLength of recieved data is 4397\u001b[0m\n",
      "\u001b[34mdone\u001b[0m\n",
      "\u001b[34m== The entire request object ==\u001b[0m\n",
      "\u001b[34m<Request 'http://169.254.255.131:8080/invocations' [POST]>\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [22/Aug/2023:05:56:51 +0000] \"POST /invocations HTTP/1.1\" 200 10744876 \"-\" \"Go-http-client/1.1\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "transformer = model.transformer(1,batch_transform_inference_instance_type,output_path=\"s3://\"+common_prefix+\"/\"+batch_inference_output_prefix+\"/\")\n",
    "transformer.transform(transform_input,content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be426131",
   "metadata": {
    "id": "be426131"
   },
   "source": [
    "<a name=\"4.-Clean-up\"></a>\n",
    "### 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e8239",
   "metadata": {
    "id": "b67e8239"
   },
   "source": [
    "<a name=\"A.-Delete-the-model\"></a>\n",
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3a3f67c",
   "metadata": {
    "id": "d3a3f67c",
    "outputId": "4c57f121-299d-4b33-8733-28492b72febe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: enova-text-speech-synthesis-2023-08-22-05-44-17-869\n"
     ]
    }
   ],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8053b",
   "metadata": {
    "id": "6fe8053b"
   },
   "source": [
    "<a name=\"B.-Unsubscribe-to-the-listing-(optional)\"></a>\n",
    "#### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0ae0e",
   "metadata": {
    "id": "c0c0ae0e"
   },
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model.\n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
